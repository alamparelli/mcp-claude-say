I Built a Voice Interface for Claude Code

The Experiment

What if you could talk to your AI coding assistant instead of typing?

I've been using Claude Code daily for months. It's become my go-to tool for navigating codebases, debugging, and writing code. But there was always friction: typing out explanations, describing bugs, asking questions.

So I built mcp-claude-say — an experiment to add voice interaction to Claude Code.


How It Works

The project uses two MCP (Model Context Protocol) servers that work together:

claude-say handles text-to-speech. When Claude responds, it speaks the answer out loud using macOS native speech synthesis. No cloud API, no latency — just instant voice output.

claude-listen handles speech-to-text. Press a hotkey, speak your question, press again. Your voice is transcribed locally using Parakeet MLX, optimized for Apple Silicon.

The result is a complete voice loop. You talk, Claude listens. Claude responds, you hear it.


Why Voice?

Three reasons drove this experiment:

1. Multitasking. I can look at code on screen while explaining a problem out loud. No context switching between keyboard and display.

2. Natural expression. Some things are easier to explain verbally. "This function feels wrong" is faster to say than to type, and often leads to better debugging conversations.

3. Accessibility. Voice interaction opens coding assistance to more people and more contexts.


The Technical Choices

Everything runs locally. I chose Parakeet MLX for transcription because it's fast (~60x real-time) and optimized for Apple Silicon. No audio leaves your machine.

For speech output, macOS native synthesis keeps things simple and responsive. Sub-100ms latency means conversations feel natural.

The Push-to-Talk approach was intentional. Automatic voice detection sounds futuristic but creates problems — false triggers, feedback loops, awkward silences. PTT gives you control.


What I Learned

Voice changes how you interact with AI. You explain more context. You think out loud. The conversation becomes collaborative rather than transactional.

It's also surprisingly effective for learning. Hearing explanations while looking at code creates a different kind of understanding than reading text.

But it's not perfect. Long technical explanations can be tedious to listen to. Code snippets need to stay on screen — you can't read code aloud. Voice works best for discussion, not documentation.


Try It Yourself

The project is open source: github.com/alamparelli/mcp-claude-say

Requirements:
- macOS with Apple Silicon
- Claude Code CLI
- A microphone

Installation is one command. Type /conversation and start talking.

This is an experiment, not a product. The code is simple, the approach is minimal. I'm sharing it because I think voice interaction with AI coding tools is worth exploring.

If you try it, let me know what works and what doesn't. The future of AI-assisted coding might be more conversational than we think.
